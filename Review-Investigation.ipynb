{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Reviews on Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Now that you are familiar with NLP, let's analyze the reviews of Olist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Run the following cell to load the reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>length_review</th>\n",
       "      <th>review_score</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7bc2406110b926393aa56f80a40eba40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>73fc7af87114b39712e6da79b0a377eb</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-18 00:00:00</td>\n",
       "      <td>2018-01-18 21:46:59</td>\n",
       "      <td>41dcb106f807e993532d446263290104</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-01-11 15:30:49</td>\n",
       "      <td>2018-01-11 15:47:59</td>\n",
       "      <td>2018-01-12 21:57:22</td>\n",
       "      <td>2018-01-17 18:42:41</td>\n",
       "      <td>2018-02-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>80e641a11e56f04c1ad469d5645fdfde</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>a548910a1c6147796b98fdf73dbeba33</td>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-10 00:00:00</td>\n",
       "      <td>2018-03-11 03:05:13</td>\n",
       "      <td>8a2e7ef9053dea531e4dc76bd6d853e6</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-28 12:25:19</td>\n",
       "      <td>2018-02-28 12:48:39</td>\n",
       "      <td>2018-03-02 19:08:15</td>\n",
       "      <td>2018-03-09 23:17:20</td>\n",
       "      <td>2018-03-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>228ce5500dc1d8e020d8d1322874b6f0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-17 00:00:00</td>\n",
       "      <td>2018-02-18 14:36:24</td>\n",
       "      <td>e226dfed6544df5b7b87a48208690feb</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-03 09:56:22</td>\n",
       "      <td>2018-02-03 10:33:41</td>\n",
       "      <td>2018-02-06 16:18:28</td>\n",
       "      <td>2018-02-16 17:28:48</td>\n",
       "      <td>2018-03-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>ferramentas_jardim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>2017-04-21 00:00:00</td>\n",
       "      <td>2017-04-21 22:02:06</td>\n",
       "      <td>de6dff97e5f1ba84a3cd9a3bc97df5f6</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-04-09 17:41:13</td>\n",
       "      <td>2017-04-09 17:55:19</td>\n",
       "      <td>2017-04-10 14:24:47</td>\n",
       "      <td>2017-04-20 09:08:35</td>\n",
       "      <td>2017-05-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parab√©ns lojas lannister adorei comprar pela I...</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "      <td>5986b333ca0d44534a156a52a8e33a83</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-10 10:59:03</td>\n",
       "      <td>2018-02-10 15:48:21</td>\n",
       "      <td>2018-02-15 19:36:14</td>\n",
       "      <td>2018-02-28 16:33:35</td>\n",
       "      <td>2018-03-09 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                         review_id  length_review  review_score  \\\n",
       "0           0  7bc2406110b926393aa56f80a40eba40              0             4   \n",
       "1           1  80e641a11e56f04c1ad469d5645fdfde              0             5   \n",
       "2           2  228ce5500dc1d8e020d8d1322874b6f0              0             5   \n",
       "3           3  e64fb393e7b32834bb789ff8bb30750e             37             5   \n",
       "4           4  f7c4243c7fe1938f181bec41a392bdeb            100             5   \n",
       "\n",
       "                           order_id   product_category_name  \\\n",
       "0  73fc7af87114b39712e6da79b0a377eb           esporte_lazer   \n",
       "1  a548910a1c6147796b98fdf73dbeba33  informatica_acessorios   \n",
       "2  f9e4b658b201a9f2ecdecbb34bed034b  informatica_acessorios   \n",
       "3  658677c97b385a9be170737859d3511b      ferramentas_jardim   \n",
       "4  8e6bfb81e283fa7e4f11123a3fb894f1           esporte_lazer   \n",
       "\n",
       "  review_comment_title                             review_comment_message  \\\n",
       "0                  NaN                                                NaN   \n",
       "1                  NaN                                                NaN   \n",
       "2                  NaN                                                NaN   \n",
       "3                  NaN              Recebi bem antes do prazo estipulado.   \n",
       "4                  NaN  Parab√©ns lojas lannister adorei comprar pela I...   \n",
       "\n",
       "  review_creation_date review_answer_timestamp  \\\n",
       "0  2018-01-18 00:00:00     2018-01-18 21:46:59   \n",
       "1  2018-03-10 00:00:00     2018-03-11 03:05:13   \n",
       "2  2018-02-17 00:00:00     2018-02-18 14:36:24   \n",
       "3  2017-04-21 00:00:00     2017-04-21 22:02:06   \n",
       "4  2018-03-01 00:00:00     2018-03-02 10:26:53   \n",
       "\n",
       "                        customer_id order_status order_purchase_timestamp  \\\n",
       "0  41dcb106f807e993532d446263290104    delivered      2018-01-11 15:30:49   \n",
       "1  8a2e7ef9053dea531e4dc76bd6d853e6    delivered      2018-02-28 12:25:19   \n",
       "2  e226dfed6544df5b7b87a48208690feb    delivered      2018-02-03 09:56:22   \n",
       "3  de6dff97e5f1ba84a3cd9a3bc97df5f6    delivered      2017-04-09 17:41:13   \n",
       "4  5986b333ca0d44534a156a52a8e33a83    delivered      2018-02-10 10:59:03   \n",
       "\n",
       "     order_approved_at order_delivered_carrier_date  \\\n",
       "0  2018-01-11 15:47:59          2018-01-12 21:57:22   \n",
       "1  2018-02-28 12:48:39          2018-03-02 19:08:15   \n",
       "2  2018-02-03 10:33:41          2018-02-06 16:18:28   \n",
       "3  2017-04-09 17:55:19          2017-04-10 14:24:47   \n",
       "4  2018-02-10 15:48:21          2018-02-15 19:36:14   \n",
       "\n",
       "  order_delivered_customer_date order_estimated_delivery_date  \n",
       "0           2018-01-17 18:42:41           2018-02-02 00:00:00  \n",
       "1           2018-03-09 23:17:20           2018-03-14 00:00:00  \n",
       "2           2018-02-16 17:28:48           2018-03-09 00:00:00  \n",
       "3           2017-04-20 09:08:35           2017-05-10 00:00:00  \n",
       "4           2018-02-28 16:33:35           2018-03-09 00:00:00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ml_olist_nlp_reviews.csv\"\n",
    "df = pd.read_csv(url, low_memory = False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98657, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40439, 17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~df['review_comment_message'].isna()]\n",
    "df = df[~df['review_score'].isna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def prepocessing(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    # stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    # tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "    #     w for w in tokenized_sentence if not w in stop_words\n",
    "    # ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word) \n",
    "        for word in tokenized_sentence\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review_comment_message'].map(prepocessing)\n",
    "y = df['review_score'].map(lambda x:0 if x>=4 else 1)\n",
    "\n",
    "X = X[y==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ante do</th>\n",
       "      <th>bem ante</th>\n",
       "      <th>chegou ante</th>\n",
       "      <th>dentro do</th>\n",
       "      <th>do prazo</th>\n",
       "      <th>muito bom</th>\n",
       "      <th>no prazo</th>\n",
       "      <th>produto chegou</th>\n",
       "      <th>produto de</th>\n",
       "      <th>produto entregue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.470656</td>\n",
       "      <td>0.750355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.711998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26413</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26414</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26415</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26416</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656582</td>\n",
       "      <td>0.418240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26417</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26418 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ante do  bem ante  chegou ante  dentro do  do prazo  muito bom  \\\n",
       "0      0.470656  0.750355          0.0   0.000000  0.464166        0.0   \n",
       "1      0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "2      0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "3      0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "4      0.711998  0.000000          0.0   0.000000  0.702181        0.0   \n",
       "...         ...       ...          ...        ...       ...        ...   \n",
       "26413  0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "26414  0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "26415  0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "26416  0.000000  0.000000          0.0   0.656582  0.418240        0.0   \n",
       "26417  0.000000  0.000000          0.0   0.000000  0.000000        0.0   \n",
       "\n",
       "       no prazo  produto chegou  produto de  produto entregue  \n",
       "0           0.0        0.000000         0.0               0.0  \n",
       "1           0.0        0.000000         0.0               0.0  \n",
       "2           0.0        0.000000         0.0               0.0  \n",
       "3           0.0        0.000000         0.0               0.0  \n",
       "4           0.0        0.000000         0.0               0.0  \n",
       "...         ...             ...         ...               ...  \n",
       "26413       0.0        0.000000         0.0               0.0  \n",
       "26414       0.0        0.000000         0.0               0.0  \n",
       "26415       0.0        0.000000         0.0               0.0  \n",
       "26416       0.0        0.627675         0.0               0.0  \n",
       "26417       0.0        0.000000         0.0               0.0  \n",
       "\n",
       "[26418 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(2,2),max_df=0.6,max_features=10)\n",
    "\n",
    "# Training it on the texts\n",
    "weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(X).toarray(),\n",
    "                 columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "weighted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07448302, 0.07850201, 0.074483  , 0.07639681, 0.69613516],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       ...,\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.07400571, 0.39854797, 0.07400568, 0.07589687, 0.37754377],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(2,2),max_df=0.6,max_features=10)\n",
    "\n",
    "# Training it on the texts\n",
    "weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(X).toarray(),\n",
    "                 columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# vectorized_documents\n",
    "# Instantiate the LDA \n",
    "n_components = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components, max_iter = 100)\n",
    "\n",
    "# Fit the LDA on the vectorized documents\n",
    "lda_model.fit_transform(weighted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(lda_model, vectorizer, top_words):\n",
    "    # 1. TOPIC MIXTURE OF WORDS FOR EACH TOPIC\n",
    "    topic_mixture = pd.DataFrame(lda_model.components_,\n",
    "                                 columns = vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # 2. FINDING THE TOP WORDS FOR EACH TOPIC\n",
    "    ## Number of topics\n",
    "    n_components = topic_mixture.shape[0]\n",
    "    ## Top words for each topic\n",
    "    for topic in range(n_components):\n",
    "        print(\"-\"*10)\n",
    "        print(f\"For topic {topic}, here are the the top {top_words} words with weights:\")\n",
    "        topic_df = topic_mixture.iloc[topic]\\\n",
    "                             .sort_values(ascending = False).head(top_words)\n",
    "        \n",
    "        print(round(topic_df,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "For topic 0, here are the the top 5 words with weights:\n",
      "do         6908.522\n",
      "prazo      6861.191\n",
      "ante       5235.192\n",
      "produto    5028.258\n",
      "chegou     3662.741\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "For topic 1, here are the the top 5 words with weights:\n",
      "que    2301.311\n",
      "n√£o    1895.194\n",
      "de     1703.642\n",
      "um     1316.189\n",
      "ma     1275.192\n",
      "Name: 1, dtype: float64\n",
      "----------\n",
      "For topic 2, here are the the top 5 words with weights:\n",
      "com      1925.561\n",
      "de       1237.232\n",
      "muito    1213.968\n",
      "da       1102.427\n",
      "loja     1067.190\n",
      "Name: 2, dtype: float64\n",
      "----------\n",
      "For topic 3, here are the the top 5 words with weights:\n",
      "produto      2796.627\n",
      "entrega      2240.940\n",
      "recomendo    2083.550\n",
      "tudo         2055.190\n",
      "√≥timo        1733.190\n",
      "Name: 3, dtype: float64\n",
      "----------\n",
      "For topic 4, here are the the top 5 words with weights:\n",
      "de           2175.718\n",
      "qualidade    2097.192\n",
      "produto      1623.216\n",
      "muito        1346.021\n",
      "boa          1061.192\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, vectorizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Analyse the reviews to understand what could be the causes of the bad review scores** ‚ùì\n",
    "\n",
    "This challenge is not as guided as the previous ones. But here are some questions to ask yourself:\n",
    "\n",
    "- Are all the reviews relevant ? \n",
    "- What about combining the title and the body of a review ?\n",
    "- What cleaning operations would you apply to the reviews ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üáßüá∑ Some Brazilian expressions and their translations:\n",
    "\n",
    "- `producto errado` = wrong product\n",
    "- `ainda nao` = not yet\n",
    "- `nao entregue` = not delivered\n",
    "- `nao veio` = did not come\n",
    "- `nao gostei` = did not like it\n",
    "- `produto defeito` = defective product\n",
    "- `nao functiona` = not working\n",
    "- `produto diferente` = different product\n",
    "- `pessima qualidade` = poor quality\n",
    "- `veio defeito` = came defect\n",
    "- `veio faltando` = came missing\n",
    "- `veio errado` = came wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations. Instead of reading 90K+ reviews, you were able to detect the main reasons of dissatisfactions on Olist.\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d40443c9dbbf2cf3195649e64f25150aabfa82ece985efba3e01d01dc9e452f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
